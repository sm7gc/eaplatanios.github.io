---
layout: project
title: machine translation
nav: projects
importance: 100
description: enhacing machine translation using contextual parameter generation and curriculum learning
img: /assets/img/projects/machine_translation/machine_translation.svg
---
<!-- 
<div class="alert alert-danger" role="alert">
  <b>NOTE:</b> The following article has also appeared on the <a href="https://blog.ml.cmu.edu/2019/01/14/contextual-parameter-generation-for-universal-neural-machine-translation/" target="_blank">CMU machine learning department blog</a>.
</div>

<div class="container-fluid p-0">
  <img class="img-responsive col-12" src="{{ '/assets/img/contextual_parameter_generation.svg' | prepend: site.baseurl | prepend: site.url }}" alt="overview figure">
  <h6 class="font-italic text-center" style="color: #78909c;"><u>Figure 1:</u> Overview of the contextual parameter generator that is introduced in this post. The top part of the figure shows a typical neural machine translation system (consisting of an encoder and a decoder network). The bottom part, shown in red, shows our parameter generator component.</h6>
</div>

<div class="col mt-4 p-0">
  NMT systems typically consist of two main components: the encoder and the decoder. The encoder takes as input the source language sentence and generates some latent representation for it (e.g., a fixed-size vector). The decoder then takes that latent representation as input and generates a sentence in the target language. The sentence generation is often done in an autoregressive manner (i.e., words are generated one-by-one in a left-to-right manner).
</div>

<h3 class="title mt-4 p-0 text-left">Multilingual Machine Translation</h3> -->